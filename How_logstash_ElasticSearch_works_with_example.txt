
How Logstash Works with OpenShift Logs..?

Overview of the Flow
----------------------
Filebeat (Log Ship Agent): Filebeat is the first tool involved. It runs on each node (or service) where logs are generated, collects logs from OpenShift pods or containers, and sends them to Logstash for further processing.
Logstash (Log Processor): Logstash processes the logs received from Filebeat (or directly from OCP if configured). It applies parsing, filtering, transformation, and enrichment rules to these logs. After processing, it sends the logs to Elasticsearch for indexing.
Elasticsearch: Elasticsearch indexes the logs, making them searchable and ready for visualization in Kibana.
Kibana: After Elasticsearch indexes the logs, you can use Kibana to visualize and analyze them.




Steps in Detail:
-----------------
1. Log Collection with Filebeat
Filebeat runs on each node in OpenShift (OCP) and listens for log files generated by OCP services or containers.

These log files can be stored in different locations like:
/var/log/containers/
/var/log/pods/

Custom log directories (if configured by the DevOps team).

Filebeat reads these logs and sends them to Logstash over the network (using TCP/UDP or HTTP).

Hereâ€™s an example of Filebeat configuration that would collect container logs:
filebeat.yaml
---------------
filebeat.inputs:
  - type: log
    paths:
      - /var/log/containers/*.log
    json:
      keys_under_root: true
      add_error_key: true
    multiline:
      pattern: '^\{'
      negate: true
      match: after
json: Specifies that the log entries are in JSON format (this makes it easier to parse).

multiline: If your logs span multiple lines (for example, stack traces), this setting ensures that the multiline logs are grouped together.




2. Logstash Configuration: Processing Logs
Once Filebeat forwards the logs to Logstash, Logstash uses a configuration file to determine how to process, parse, and filter these logs.

A Logstash pipeline typically consists of three main sections:

Input: Specifies the source of the logs (e.g., Filebeat or direct log sources).
Filter: Defines how to process and transform the logs.
Output: Specifies where the processed logs should be sent (in your case, Elasticsearch).

logstash.config
------------------------------------------------------
input {
  beats {
    port => 5044
  }
}

filter {
  # Parse JSON logs (if the logs are in JSON format)
  json {
    source => "message"
  }

  # Add fields for Kubernetes metadata
  mutate {
    add_field => {
      "kubernetes.pod_name" => "%{[kubernetes][pod][name]}"
      "kubernetes.namespace" => "%{[kubernetes][namespace]}"
      "kubernetes.container_name" => "%{[kubernetes][container][name]}"
    }
  }

  # If there is a log level field, make sure it's downcased for consistency
  mutate {
    lowercase => ["log.level"]
  }

  # Drop logs that are not necessary (for example, debug logs)
  if [log.level] == "debug" {
    drop { }
  }
}

output {
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "ocp-logs-%{+YYYY.MM.dd}"
    user => "elastic"
    password => "password"
  }
}


---------------------------------------------------------------------
Detailed Breakdown of Logstash Pipeline:
1. Input Section:
beats input listens for logs coming from Filebeat (configured to send logs to Logstash via TCP/UDP or HTTP).
The logs are then processed one by one in the Logstash pipeline.

2. Filter Section:
json filter: Since Filebeat sends logs as JSON (as per the previous example), the json filter is used to parse the JSON-encoded log message.

Example log in json format....
--------------------------------------
{
  "log": "Request failed",
  "timestamp": "2025-05-12T12:34:56Z",
  "level": "ERROR",
  "kubernetes": {
    "pod": {"name": "my-app-123"},
    "namespace": "default",
    "container": {"name": "my-app-container"}
  }
}

The json filter parses this log and converts it into fields that Logstash can process (e.g., log, timestamp, level, kubernetes.pod_name, etc.).

mutate filter:
-------------------
Adds custom fields to the log, like the pod name, namespace, and container name based on the Kubernetes metadata.
Ensures fields like log.level are converted to lowercase to maintain consistency in searches.


The filter checks if the log level is "debug" and drops those logs if they are not needed in the production environment.



3. Output Section:
Elasticsearch Output:

The logs are sent to Elasticsearch. The index name can be dynamic based on the date (e.g., ocp-logs-2025.05.12).

index: Defines the index name pattern where logs will be stored.
Authentication: In production, you may need to provide credentials to access Elasticsearch (like user and password).

Example Logs After Processing:
Once the logs have passed through Filebeat, Logstash, and are indexed in Elasticsearch, you will have structured logs that look like this (in Elasticsearch):

{
  "@timestamp": "2025-05-12T12:34:56Z",
  "log": "Request failed",
  "level": "ERROR",
  "kubernetes.pod_name": "my-app-123",
  "kubernetes.namespace": "default",
  "kubernetes.container_name": "my-app-container",
  "message": "Request failed due to timeout",
  "source_ip": "192.168.1.1"
}



These fields are now available for querying and visualizing in Kibana.



--------------------------------------------

Visualizations in Kibana:
Once the logs are indexed in Elasticsearch, you can create visualizations based on fields such as:

Log Level (e.g., ERROR, WARN, INFO): To monitor the types of logs.
Pod Name: To see logs per pod.
Namespace: To filter logs by namespace.
Timestamp: To create time-based charts (e.g., log frequency over time).


Example visualization:

Bar Chart showing the number of ERROR logs per pod.
Line Chart to show the trend of logs over time.
Log Format: Confirm if logs are in JSON format for easy parsing by Logstash.

